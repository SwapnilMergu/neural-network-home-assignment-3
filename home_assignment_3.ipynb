{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIvDwNb1rIRfDBLVJGjWfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwapnilMergu/neural-network-home-assignment-3/blob/main/home_assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFWVVYhqerZd",
        "outputId": "f2b755a8-d515-429d-8519-8bb5991e6e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - loss: 3.8152\n",
            "Epoch 2/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 3.1154\n",
            "Epoch 3/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - loss: 2.9136\n",
            "Epoch 4/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - loss: 2.6701\n",
            "Epoch 5/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - loss: 2.4889\n",
            "Epoch 6/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 2.3726\n",
            "Epoch 7/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 2.2852\n",
            "Epoch 8/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 2.2206\n",
            "Epoch 9/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - loss: 2.1581\n",
            "Epoch 10/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 2.1093\n",
            "Epoch 11/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 2.0683\n",
            "Epoch 12/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 2.0220\n",
            "Epoch 13/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 1.9843\n",
            "Epoch 14/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 1.9451\n",
            "Epoch 15/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 1.9145\n",
            "Epoch 16/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 1.8769\n",
            "Epoch 17/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.8472\n",
            "Epoch 18/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.8350\n",
            "Epoch 19/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 1.8013\n",
            "Epoch 20/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 1.7854\n",
            "That time of year in’d mj(bow,\n",
            " th  tut py meerd:\n",
            "Bjy withes f fareye me ve than t t d  t   t gh ve  Mesort bize t  p’mery him:Ores ist fasunor chach Hand,\n",
            "We I\n",
            "Whthancarun t s  sth le ccunt st pise k, Thanostwh podey\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "\n",
        "#Load a text dataset (using a small sample of Shakespeare) Shakespeare Sonnets\n",
        "url = \"https://www.gutenberg.org/files/1041/1041-0.txt\"\n",
        "text_path = tf.keras.utils.get_file(\"shakespeare.txt\", url)\n",
        "text = open(text_path, \"r\", encoding='utf-8').read()\n",
        "\n",
        "#Take a small portion for faster training\n",
        "text = text[:100000]\n",
        "\n",
        "#Preprocess: map characters to integers\n",
        "chars = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(chars)}\n",
        "idx2char = np.array(chars)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Define sequence length\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // seq_length\n",
        "\n",
        "# Create input-target sequences\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch and shuffle\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 100000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#Define the LSTM RNN model\n",
        "vocab_size = len(chars)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "#Compile and train\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "#Train for few epochs\n",
        "EPOCHS = 20\n",
        "model.fit(dataset, epochs=EPOCHS)\n",
        "\n",
        "#Generate text\n",
        "def generate_text(model, start_string, temperature=1.0, num_generate=200):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :] / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "#Example text generation\n",
        "print(generate_text(model, start_string=\"That time of year\", temperature=0.8))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "#Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def nlp_preprocess(sentence):\n",
        "    #1. Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(\"Original Tokens:\", tokens)\n",
        "\n",
        "    #2. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stop = [word for word in tokens if word.lower() not in stop_words]\n",
        "    print(\"Tokens Without Stopwords:\", tokens_no_stop)\n",
        "\n",
        "    #3. Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens_no_stop]\n",
        "    print(\"Stemmed Words:\", stemmed_tokens)\n",
        "\n",
        "#Example sentence\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "nlp_preprocess(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVHfzeFz9kBF",
        "outputId": "fd204638-7ad5-49cd-8bf6-c303a32640ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.']\n",
            "Tokens Without Stopwords: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri', '.']\n",
            "Stemmed Words: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "#Load spaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#Input sentence\n",
        "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "\n",
        "#Process the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "#Extract and print named entities\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Text: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhlINoK5-R1k",
        "outputId": "ccf0ef74-6e7e-40ca-a027-8036a19f28dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Text: Barack Obama, Label: PERSON, Start: 0, End: 12\n",
            "\n",
            "Text: 44th, Label: ORDINAL, Start: 27, End: 31\n",
            "\n",
            "Text: the United States, Label: GPE, Start: 45, End: 62\n",
            "\n",
            "Text: the Nobel Peace Prize, Label: WORK_OF_ART, Start: 71, End: 92\n",
            "\n",
            "Text: 2009, Label: DATE, Start: 96, End: 100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    #Dot product of Q and K^T\n",
        "    dk = Q.shape[-1]\n",
        "    scores = np.dot(Q, K.T)\n",
        "    #Scale by sqrt(dk)\n",
        "    scaled_scores = scores / np.sqrt(dk)\n",
        "    #Apply softmax to get attention weights\n",
        "    def softmax(x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "    attention_weights = softmax(scaled_scores)\n",
        "    #Multiply by V to get output\n",
        "    output = np.dot(attention_weights, V)\n",
        "    return attention_weights, output\n",
        "#Input matrices\n",
        "Q = np.array([[1, 0, 1, 0],\n",
        "              [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0],\n",
        "              [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4],\n",
        "              [5, 6, 7, 8]])\n",
        "\n",
        "#Run the function\n",
        "weights, output = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "#Print results\n",
        "print(\"Attention Weights:\\n\", weights)\n",
        "print(\"\\nOutput:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxMFNpm2DXup",
        "outputId": "0235fc09-f46f-4f46-81fc-77203b7a6167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "\n",
            "Output:\n",
            " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Input sentence\n",
        "text = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
        "\n",
        "# Analyze sentiment\n",
        "result = sentiment_pipeline(text)[0]\n",
        "\n",
        "# Print the result\n",
        "\n",
        "print(\"Sentiment:\", result['label'])\n",
        "print(\"Confidence Score:\", round(result['score'], 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcHUIruiG5iw",
        "outputId": "06ee4eb5-1063-464b-d1b5-14f6a99ee1a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n"
          ]
        }
      ]
    }
  ]
}